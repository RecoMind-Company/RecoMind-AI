# src/recomind/data_embedding/core/pipeline.py

from .database_scanner import DatabaseScanner
from .description_generator import DescriptionGenerator
from .vector_store import VectorStore
from shared import api_client
from . import embedding_config 
# import time # Not needed here, assuming 'DescriptionGenerator' handles the sleep internally

def run_ingestion_pipeline():
    """
    Orchestrates the entire ingestion process from data scanning to vector saving.
    """
    print("--- Starting Ingestion Pipeline ---")
    
    # Step 1: Fetch database connection settings
    source_settings = api_client.fetch_source_db_settings()

    if not source_settings:
        print("Pipeline aborted: Failed to fetch database connection settings.")
        return
    
    # Step 2: Save the fetched settings for auditing
    print("Attempting to save the fetched settings for audit...")
    api_client.save_settings_to_db(source_settings)

    print(f"DEBUG: Company ID {source_settings['company_id']} settings loaded.")
    
    # Step 3: Scan tables
    scanner = DatabaseScanner(db_settings=source_settings)
    tables_data = scanner.scan_tables()

    if not tables_data:
        print("Pipeline aborted: No table data was found by the scanner.")
        return

    # Step 4: Generate descriptions using LLM (Sleep must be handled internally to avoid 429 errors)
    generator = DescriptionGenerator()
    data_with_descriptions = generator.generate_for_tables(tables_data, source_settings)

    if not data_with_descriptions:
        print("Pipeline aborted: No descriptions were generated by the LLM.")
        return

    # Step 5: Save vectors to the vector store
    vector_db = VectorStore()
    vector_db.save(data_with_descriptions)

    print("\n--- Ingestion Pipeline Completed Successfully! ---")


if __name__ == "__main__":
    run_ingestion_pipeline()