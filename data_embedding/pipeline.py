# embedding/pipeline.py

from database_scanner import DatabaseScanner
from description_generator import DescriptionGenerator
from vector_store import VectorStore
import config

def run_ingestion_pipeline():
    """
    The main function that orchestrates the entire ingestion process.
    1. Scans the source database to get table schemas.
    2. Generates business descriptions for those schemas using an LLM.
    3. Saves the final data, including vector embeddings, to the destination DB.
    """
    print(f"DEBUG: API Key Loaded: |{config.OPENROUTER_API_KEY}|")
    
    print("--- Starting Ingestion Pipeline ---")

    # Step 1: Scan the source database to get schemas
    scanner = DatabaseScanner()
    tables_data = scanner.scan_tables()

    if not tables_data:
        print("Pipeline aborted: No table data was found by the scanner.")
        return

    # Step 2: Generate descriptions for the schemas using an LLM
    generator = DescriptionGenerator()
    data_with_descriptions = generator.generate_for_tables(tables_data)

    if not data_with_descriptions:
        print("Pipeline aborted: No descriptions were generated by the LLM.")
        return

    # Step 3: Save the final data to the vector store
    vector_db = VectorStore()
    vector_db.save(data_with_descriptions)

    print("\n--- Ingestion Pipeline Completed Successfully! ---")

if __name__ == "__main__":
    # This block allows the script to be run directly from the command line
    run_ingestion_pipeline()